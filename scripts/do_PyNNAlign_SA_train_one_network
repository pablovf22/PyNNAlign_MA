#!/bin/bash
#SBATCH --job-name=PyNNAlign_SA_HLA-DR_05_260226
#SBATCH --output=/home/projects2/s243357/master_thesis/PyNNAlign_MA/outputs/logs/slurm-%x.out
#SBATCH --error=/home/projects2/s243357/master_thesis/PyNNAlign_MA/outputs/logs/slurm-%x.err
#SBATCH --nodes=1
#SBATCH --partition=gpu
#SBATCH --mem=15GB
#SBATCH --cpus-per-task=4
#SBATCH --time=24:00:00

RUN_ID="PyNNAlign_SA_HLA-DR_05_260226"

ENV="PyNNAlign_MA"

PROJECT_ROOT="/home/projects2/s243357/master_thesis/PyNNAlign_MA"
TRAIN_EL="$PROJECT_ROOT/data/f000_el_hla_dr"
VAL_EL="$PROJECT_ROOT/data/c000_el_hla_dr"
PSEUDO="$PROJECT_ROOT/data/pseudosequence.2023.dat"
BLF="$PROJECT_ROOT/data/BLOSUM50"
SYN="$PROJECT_ROOT/models/$RUN_ID.pt"
TRAINING_CURVES="$PROJECT_ROOT/outputs/plots/$RUN_ID.png"

mkdir -p "$PROJECT_ROOT/outputs/wandb/$RUN_ID"

WB_DIR="$PROJECT_ROOT/outputs/wandb/$RUN_ID"

BATCH_SIZE=256
BURN_IN_SA=100
N_HIDDEN=56
LEARNING_RATE=0.00025
EPOCHS=100
ACTIVATION="tanh"
CRITERION="mse"
OPTIMIZER="sgd"


cd "$PROJECT_ROOT"

source $(conda info --base)/etc/profile.d/conda.sh
conda activate "$ENV"

START_TIME=$(date +%s)


python pyscripts/PyNNAlign_SA_train_one_network.py -b "$BATCH_SIZE" -sa "$BURN_IN_SA" -tr "$TRAIN_EL" -bl "$BLF" -ps "$PSEUDO" -syn "$SYN" -nh "$N_HIDDEN" -lr "$LEARNING_RATE" -e "$EPOCHS" -val "$VAL_EL" -tc "$TRAINING_CURVES" -wb "$RUN_ID" -w "$WB_DIR" -a $ACTIVATION -c "$CRITERION" -o "$OPTIMIZER" > "$PROJECT_ROOT/outputs/logs/$RUN_ID.log" 2>&1


END_TIME=$(date +%s)
ELAPSED=$((END_TIME - START_TIME))

echo "Total runtime (seconds): $ELAPSED"
